{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be69a762-03d4-46ff-8347-9cf158ebe0fa",
   "metadata": {},
   "source": [
    "# 선형분류기의 한계\n",
    "\n",
    "AND, OR 연산(선형 문제)에 대해서는 기존 머신러닝으로 가능했지만, XOR(비선형 문제)에 대해서는 좌표상의 직선을 그리는 방식으로는 분류가 불가능. \n",
    "\n",
    "( 의사결정나무나 랜덤포레스트가 성능이 높게 나오는 이유이기도 함, AND OR 연산을 하는 직선(=branch)을 엄청 많이 그리기 때문 )\n",
    "\n",
    "( 의사결정나무에 쓰이는 특성을 무작위로 선정하여 여러 번 반복하면 랜덤포레스트가 됨, 무작위 선정을 반복하여 최적의 변수를 찾아내는 방법 )\n",
    "\n",
    "활성화 함수를 사용하면 입력값에 대한 출력이 비선형으로 출력되게 됨. 즉 퍼셉트론을 활용한 신경망은 non-linear layer를 여러 개 쌓아서 완성됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb9aad-11a4-4697-a5b9-0f406769f1bf",
   "metadata": {},
   "source": [
    "# 활성화 함수의 종류\n",
    "\n",
    "## 시그모이드 ( sigmoid )\n",
    "  - 출력값을 0에서 1 사이로 만들어준다 ( 데이터의 평균은 0.5가 됨 )\n",
    "  - 따라서 train_data 와 train_label 이 1:1로 매칭하는 binary classification의 마지막 층(출력층)에 0~1 사이 값을 만들고 싶을 때 사용한다.\n",
    "  - 0.5 이상이면 1, 미만이면 0으로 받아도 되는 경우에 사용한다.\n",
    "  - 위의 특별한 경우를 제외하고 대부분의 경우에는 좋은 성능이 나올 수 없으므로 사용되지 않는다.\n",
    "  - 시그모이드의 문제점은 0에 가까운 값들이 역전파될수록, 기울기가 0이 되어 사라지므로 학습되지 않는 현상이 생긴다. ( Vanishing gradient )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c1bbf-ee02-4b16-b0c4-bf3c6d59e527",
   "metadata": {},
   "source": [
    "## 탄 ( Tanh )\n",
    "\n",
    "- 시그모이드와 유사하지만, 값의 범위가 -1 ~ 1 이다 ( 데이터의 평균은 0이 됨 )\n",
    "- 대부분의 경우 시그모이드보다 Tanh이 성능이 더 좋음\n",
    "- 시그모이드와 마찬가지로 Vanishing gradient 현상이 일어남"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1c52d-fec8-46c7-837b-32273fe9fdcf",
   "metadata": {},
   "source": [
    "## 렐루 ( ReLU )\n",
    "- 일반적으로 대부분 상황에 가장 성능이 좋음\n",
    "- 대부분 input 값에 대해 기울기가 0이 아니기 때문에 학습이 빠름\n",
    "- ( 학습을 느리게 하는 원인이 gradient가 0이 되는 것인데, 이를 막아줌 )\n",
    "- 함수 그래프만 보면 최소값이 0이기 때문에 input이 0보다 작을 경우 기울기가 0이 될 거라 생각하지만, 은닉층의 대부분 노드값들은 0보다 크기 때문에 기울기가 0이 되는 경우는 많지 않음\n",
    "- 값이 음수일 때 기울기가 0인 것이 단점이라면 단점이지만, 거의 무시할 수 있는 수준으로 학습이 잘 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41b8ad-9c9e-4a18-8c3b-eb4abd26d802",
   "metadata": {},
   "source": [
    "## 리키 렐루 ( leaky ReLU )\n",
    "- ReLU와 유일한 차이점은 input값이 음수일 때 기울기가 0이 아닌 0.01을 갖게 함\n",
    "- 즉 해당 층의 최소값은 0.01이 됨\n",
    "- 일반적으로 많이 쓰진 않지만, ReLu의 성능이 부족할 때 시도해볼 수 있음\n",
    "- ReLU보다 학습이 잘 되는 경향이 있긴 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d8a31-449d-4345-9e18-ef83cfbd36b5",
   "metadata": {},
   "source": [
    "# 역전파 알고리즘\n",
    "- 활성화 함수를 이용하여 비선형 문제를 해결할 수 있었지만, layer가 많아질수록 파라미터가 급증하게 되고 파라미터를 적절하게 학습하는 것이 어려웠음.\n",
    "- 출력값에 대한 입력값의 기울기(미분값)을 출력층 layer에서부터 계산하여 거꾸로 전파시키는 것이다. 이렇게 거꾸로 전파시켜서 최종적으로 출력층에서의 output값에 대한 입력층에서의 input data의 기울기 값을 구할 수 있다.\n",
    "- 출력층 바로 전 layer에서부터 기울기(미분값)을 계산하고 이를 점점 거꾸로 전파시키면서 전 layer들에서의 기울기와 서로 곱하는 식으로 나아가면 최종적으로 출력층의 output에 대한 입력층의 input 기울기를 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3d312-989a-4ebb-8606-b86efd95c99c",
   "metadata": {},
   "source": [
    "# 경사하강법 ( Gradient descent )\n",
    "- 역전파 알고리즘을 통해 파라미터가 많고 layer가 여러 개 있을 때 각 layer에서의 기울기 값을 구할 수 있었음\n",
    "- 구해낸 기울기 값을 이용하여 경사하강법을 통해 가중치w와 b를 update시키면서 파라미터가 급증하게 되는 문제를 해결하게 됨\n",
    "- 각 layer의 node(parameter)별로 학습을 해야하기 때문에 각 layer의 node별로 기울기 값을 계산해야함.\n",
    "- 결과적으로 역전파가 이뤄질 때마다 각 레이어의 parameter들이 업데이트되므로 파라미터의 수가 증가하는 문제를 해결할 수 있게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d043a-32c6-4090-88d6-c922057891dd",
   "metadata": {},
   "source": [
    "# 배치 사이즈에 대하여 ( Batch size )\n",
    "모델이나 상황 별로 다르겠지만 일반적으로 배치 사이즈가 클 경우 최대 정확도가 높게 나온다. 반면 배치 사이즈가 작을 경우 정확도의 분산이 줄어들어 좀 더 정확도가 안정적이게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c9d76-16bf-49e5-a33f-fbfce046abcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
