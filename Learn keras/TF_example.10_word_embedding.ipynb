{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16aec7e-0f75-4b43-94ef-75c9e207ed93",
   "metadata": {},
   "source": [
    "# 단어 임베딩 사용하기\n",
    "단어와 벡터를 연관짓는 강력하고 인기 있는 또 다른 방법은 단어 임베딩이라는 밀집 단어 벡터를 사용하는 것입니다. 원핫 인코딩으로 만든 벡터는 희소하고(대부분 0으로 채워지므로) 고차원입니다(어휘 사전의 단어 수만큼 차원을 만들어야합니다). 반면 단어 임베딩은 저차원의 실수형 벡터입니다(희소 벡터의 반대인 밀집 벡터입니다). 원핫 인코딩으로 얻은 단어 벡터와 달리 단어 임베딩은 데이터로부터 학습됩니다. 보통 256차원, 512차원 또는 큰 어휘 사전을 다룰 때는 1,024차원의 단어 임베딩을 사용합니다. 반면 원핫 인코딩은 20,000차원 이상의 벡터일 경우가 많습니다. 따라서 단어 임베딩이 더 많은 정보를 적은 차원에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb5f8b-f40a-47b3-974f-6210311a5bb4",
   "metadata": {},
   "source": [
    "단어 임베딩을 만드는 방법은 두 가지입니다.\n",
    "- (문서 분류나 감성 예측과 같은) 관심 대상인 문제와 함께 단어 임베딩을 학습합니다. 이런 경우에는 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습합니다.\n",
    "- 풀려는 문제가 아니고 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드합니다. 이를 사전 훈련된 단어 임베딩이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c311b58e-0f98-46a0-a03f-8f24e36bd814",
   "metadata": {},
   "source": [
    "# 임베딩 층을 사용하여 단어 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d13c95-4bc4-4c87-a514-a79360d173ef",
   "metadata": {},
   "source": [
    "단어와 밀집 벡터를 연관짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것입니다. 이 방식의 문제점은 임베딩 공간이 구조적이지 않다는 것입니다. 예를 들어 accurate와 exact 단어가 대부분의 문장에서 비슷한 의미로 사용되지만 완전히 다른 임베딩을 가지게 됩니다. 심층 신경망이 이런 임의의 구조적이지 않은 임베딩 공간을 이해하기는 어렵습니다.\n",
    "\n",
    "단어 벡터 사이에 조금 더 추상적이고 기하학적인 관계를 얻으려면 단어 사이에 있는 의미 관계를 반영해야합니다. 단어 임베딩은 언어를 기하학적 공간에 매핑하는 것입니다. 예를 들어 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 빅터로 임베딩될 것입니다. 일반적으로 두 단어 벡터 사이의 거리(L2 거리)는 이 단어 사이의 의미 거리와 관계되어 있습니다(멀리 떨어진 위치에 임베딩된 단어의 의미는 서로 다르고 반면 비슷한 단어들은 가까이 임베딩됩니다). 거리 외에 임베딩 공간의 특정 방향도 의미를 가질 수 있습니다.\n",
    "\n",
    "실제 단어 임베딩 공간에서 의미 있는 기하학적 변환의 일반적인 예는 '성별'벡터와 '복수'(plural)벡터입니다. 예를 들어 'king' 벡터에 'female' 벡터를 더하면 'queen' 벡터가 됩니다. 'plural' 벡터를 더하면 'kings'가 됩니다. 단어 임베딩 공간은 전형적으로 이런 해석 가능하고 잠재적으로 유용한 수천 개의 벡터를 특성으로 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ffcf0d-64af-47b5-a3b2-23de4e967347",
   "metadata": {},
   "source": [
    "사람의 언어를 완벽하게 매핑해서 어떤 자연어 처리 작업에도 사용할 수 있는 이상적인 임베딩 공간이 있을까요? 아마도 가능하겠지만 아직까지 이런 종류의 공간은 만들지 못했습니다. 사람의 언어에도 그런 것은 없습니다. 세상에는 많은 다른 언어가 있고 언어는 특정 문화와 환경을 반영하기 때문에 서로 동일하지 않습니다. 실제로 좋은 단어 임베딩 공간을 만드는 것은 문제에 따라 크게 달라집니다. 영어로 된 영화 리뷰 감성 분석 모델을 위한 완벽한 단어 임베딩 공간은 영어로 된 법률 문서 분류 모델을 위한 완벽한 임베딩 공간과 다를 것 같습니다. 특정 의미 관계의 중요성이 작업에 따라 다르기 때문입니다.\n",
    "\n",
    "따라서 새로운 작업에는 새로운 임베딩을 학습하는 것이 타당합니다. 다행히 역전파를 사용해 쉽게 만들 수 있고 케라스를 사용하면 더 쉽습니다. 임베딩 층의 가중치를 학습하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f36a5e-dcaa-41e9-a007-57e4893151b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# Embedding 층은 적어도 두 개의 매개변수를 받습니다.\n",
    "# 가능한 토큰의 개수(여기서는 1,000으로 단어 인덱스 최댓값 + 1입니다)와 임베딩 차원(64)입니다.\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8c162-719b-4fba-ac1c-1eeefaeb1b44",
   "metadata": {},
   "source": [
    "임베딩 층을 (특정 단어를 나타내는) 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해하는 것이 가장 좋습니다. 정수를 입력으로 받아 내부 딕셔너리에서 이 정수에 연관된 벡터를 찾아 반환합니다. 딕셔너리 탐색은 효율적으로 수행됩니다.\n",
    "\n",
    "임베딩 층은 크기가 (samples, sequence_length)인 2D 정수 텐서를 입력으로 받습니다. 각 샘플은 정수의 시퀀스입니다. 가변 길이의 시퀀스를 임베딩할 수 있습니다. 예를 들어 위의 임베딩 층에 (32, 10) 크기의 배치(길이가 10인 32개의 시퀀스)나 (64, 15) 크기로 이루어진 배치를 주입할 수 있습니다. 배치에 있는 모든 시퀀스는 길이가 같아야 하므로(하나의 텐서에 담아야 하기 때문에) 작은 길이의 시퀀스는 0으로 패딩되고 길이가 더 긴 시퀀스는 잘립니다. \n",
    "\n",
    "임베딩 층은 크기가 (samples, sequence_length, embedding_demensionality)인 3D 실수형 텐서를 반환합니다. 이런 3D 텐서는 RNN 층이나 1D 합성곱 층에서 처리됩니다.\n",
    "\n",
    "임베딩 층의 객체를 생성할 때 가중치(토큰 벡터를 위한 내부 딕셔너리)는 다른 층과 마찬가지로 랜덤하게 초기화됩니다. 훈련하면서 이 단어 벡터는 역전파를 통해 점차 조정되어 이어지는 모델이 사용할 수 있도록 임베딩 공간을 구성합니다. 훈련이 끝나면 임베딩 공간은 특정 문제에 특화된 구조를 많이 가지게 됩니다.\n",
    "\n",
    "이를 익숙한 IMDB 영화 리뷰 감성 예측 문제에 적용해보겠습니다. 먼저 데이터를 준비하고, 영화 리뷰에서 가장 빈도가 높은 10,000개의 단어를 추출하고 리뷰에서 20개 단어 이후는 버립니다. 이 네트워크는 10,000개 단어에 대해 8차원의 임베딩을 학습하여 정수 시퀀스 입력(2D 정수 텐서)를 임베딩 시퀀스(3D 실수형 텐서)로 바꿀 것입니다. 그 다음 이 텐서를 2D로 펼쳐서 분류를 위한 Dense층을 훈련하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f41378-3203-4795-987b-4f920a384b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# 특성으로 사용할 단어의 수\n",
    "max_features = 10000\n",
    "# 사용할 텍스트의 길이(가장 빈번한 max_features개의 단어만 사용합니다.)\n",
    "maxlen = 20\n",
    "\n",
    "# 정수 리스트로 데이터를 로드합니다.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환합니다.\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb919552-9e26-43fa-98a6-b3db2df41843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 910us/step - loss: 0.6725 - acc: 0.6201 - val_loss: 0.6324 - val_acc: 0.6906\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 777us/step - loss: 0.5559 - acc: 0.7462 - val_loss: 0.5380 - val_acc: 0.7254\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 807us/step - loss: 0.4682 - acc: 0.7853 - val_loss: 0.5041 - val_acc: 0.7454\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 765us/step - loss: 0.4223 - acc: 0.8116 - val_loss: 0.4936 - val_acc: 0.7580\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 807us/step - loss: 0.3911 - acc: 0.8259 - val_loss: 0.4928 - val_acc: 0.7606\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 764us/step - loss: 0.3657 - acc: 0.8411 - val_loss: 0.4963 - val_acc: 0.7612\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 0.3430 - acc: 0.8522 - val_loss: 0.5015 - val_acc: 0.7570\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 761us/step - loss: 0.3228 - acc: 0.8630 - val_loss: 0.5083 - val_acc: 0.7566\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 770us/step - loss: 0.3041 - acc: 0.8737 - val_loss: 0.5165 - val_acc: 0.7516\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 0.2861 - acc: 0.8834 - val_loss: 0.5255 - val_acc: 0.7512\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정합니다.\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# embedding 층의 출력 크기는 (samples, maxlen, 8)가 됩니다.\n",
    "\n",
    "# 3D 임베딩 텐서를 (samples, maxlen * 8) 크기의 2D 텐서로 펼칩니다.\n",
    "model.add(Flatten())\n",
    "\n",
    "# 분류기를 추가합니다.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 32,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542e7a7-b425-472c-9872-d0e700c87d6e",
   "metadata": {},
   "source": [
    "약 75% 정도의 검증 정확도가 나옵니다. 리뷰에서 20개의 단어만 사용한 것치고 꽤 좋은 결과입니다. 하지만 임베딩 시퀀스를 펼치고 하나의 Dense층을 훈련했으므로 입력 시퀀스에 있는 각 단어를 독립적으로 다루었습니다. 단어 사이의 관계나 문장의 구조를 고려하지 않았습니다(예를 들어 이 모델은 반어법으로 쓰인 문장에 대해 긍정적인 리뷰로 다룰 것입니다). 각 시퀀스 전체를 고려한 특성을 학습하도록 임베딩 층 위에 순환 층이나 1D 합성곱 층을 추가하는 것이 좋습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb236996-25ae-4dee-b7f8-28c59c4e29ff",
   "metadata": {},
   "source": [
    "# 사전 훈련된 단어 임베딩 사용하기\n",
    "훈련 데이터가 부족하면 작업에 맞는 단어 임베딩을 학습할 수 없습니다.\n",
    "\n",
    "이럴 때는 미리 훈련된 임베딩 층을 사용할 수 있습니다. 사전 훈련된 컨브넷을 사용한 이유처럼, 일반적인 상황에 적용시키기에 빠르고 사용하기 쉽습니다. \n",
    "\n",
    "단어 임베딩은 일반적으로 단어 출현 통계를 사용하여 계산됩니다. 여기에는 여러 가지 기법이 사용되는데 신경망을 사용하는 것도 있고 그렇지 않은 방법도 있습니다. 단어를 위해 밀집된 저차원 임베딩 공간을 비지도 학습 방법으로 계산하는 아이디어가 사용되기 시작한 것은 Word2vec 알고리즘이 등장한 이훙비니다. 이 알고리즘은 성별 같은 구체적인 의미가 있는 속성을 잡아냅니다.\n",
    "\n",
    "케라스의 임베딩 층에서도 해당 Wrod2vec을 사용할 수 있습니다. 또 다른 하나는 GloVe입니다. GloVe는 단어의 동시 출현 통계를 기록한 행렬을 분해하는 기법을 사용합니다. 개발자들은 해당 임베딩 층을 위해 수백만 개의 토큰을 이미 훈련시켜놓았습니다.\n",
    "\n",
    "이번에는 GloVe를 사용해봅니다. Word2vec이나 다른 임베딩도 방법은 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e6955-967d-46a4-801b-f52f44c594e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe 임베딩 크기가 커서 내려받는 데 오래 걸려서 이후 과정 생략"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
